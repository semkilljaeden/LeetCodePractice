CAP：
    Consistency, Availability, Partition fault tolerance


Zookeeper 选举：

    每个改变Zookeeper状态的操作都会形成一个对应的zxid，并记录到transaction log中。 这个值越大，表示更新越新
    myid：服务器SID，一个数字,通过配置文件配置，唯一SID：服务器的唯一标识成为Leader的必要条件： 
    Leader要具有最高的zxid；当集群的规模是n时，集群中大多数的机器（至少n/2+1）得到响应并follow选出的Leader。
    心跳机制：Leader与Follower利用PING来感知对方的是否存活，当Leader无法相应PING时，将重新发起Leader选举。
    选举有两种情况，一是服务器启动的投票，二是运行期间的投票。
    服务器启动时期的Leader选举
        1.每个服务器发送一个投票(SID,ZXID)其中sid是自己的myid，初始阶段都将自己投为Leader。
        2.接收来自其他服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。
        3.处理投票针对每个投票都按以下规则与自己的投票PK，PK后依据情况是否更新投票，再发送给其他机器。
            a.优先检查ZXID，ZXID较大者优先为Leaderb.如果ZXID相同，检查SID，SID较大者优先为Leader
        5.统计投票每次投票后，服务器统计所有投票，判断是否有过半的机器收到相同的投票，如果某个投票达到一半的要求，则认为该投票提出者可以成为Leader。
        6.改变服务器状态一旦确定了Leader，每个服务器都更新自己的状态，Leader变更为Leading，Follower变更为Following 正常情况下一旦选出一个Leader则一直会保持，除非Leader服务器宕掉，则再进行重新选举。

    服务器运行时期的Leader选举
        1.变更状态当Leader宕机后，余下的所非Observer的服务器都会将自己的状态变更为Looking，然后开启新的Leader选举流程。
        2.每个服务器发出一个投票。生成(SID,ZXID)信息，注意运行期间的ZXID可能是不同的，但是在投票时都会将自己投为Leader，然后发送给其他的服务器。
        3.接收来自各个服务器的投票与启动时过程相同
        4.处理投票与启动时过程相同5.统计投票与启动时过程相同
        6.改变服务器状态与启动时过程相同

    用epoch来reject旧leader的信息
    领导者选举的过程中至少要有三台zkServer投了同一个zkServer，才会符合过半机制，才能选出来一个Leader。







Kakfka:

    Broker: 
        服务器

    Topic: 

    Partition: 
        Topic的多个replica，主从分离。每一个分区都有多个副本，副本的作用是做备胎。
        当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10个，
        且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。

        方便扩展。因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。

        提高并发。以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。

        熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在kafka中，如果某个topic有多个partition，
        producer又怎么知道该将数据发往哪个partition呢？kafka中有几个原则：

        partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。

        如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。

        如果既没指定partition，又没有设置key，则会轮询选出一个partition。

    Consumer:
      
    Consumer Group:

        同一个组里的consumer只能消费一个topic下面不同的partition。
        consumer在一个consumer group里面和partition个数对应。如果consumer多于partition，这个consumer收不到message

        多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，
        如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。
        如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用。

    一致性

        保证消息不丢失是一个消息队列中间件的基本保证，那producer在向kafka写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，
        那就是通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认kafka接收到数据，这个参数可设置的值为0、1、all。

        0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。

        1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功。

        all代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。

        最后要注意的是，如果往不存在的topic写数据，能不能写入成功呢？kafka会自动创建topic，分区和副本的数量根据默认配置都是1。

    消息顺序

        Kafka分布式的单位是partition，同一个partition用一个write ahead log组织，所以可以保证FIFO的顺序。不同partition之间不能保证顺序。但是绝大多数用户都可以通过message key来定义，
        因为同一个key的message可以保证只发送到同一个partition，比如说key是user id，table row id等等，所以同一个user或者同一个record的消息永远只会发送到同一个partition上，
        保证了同一个user或record的顺序。当然，如果你有key skewness 就有些麻烦，需要特殊处理

        Kafka 中发送1条消息的时候，可以指定(topic, partition, key) 3个参数。partiton 和 key 是可选的。
        如果你指定了 partition，那就是所有消息发往同1个 partition，就是有序的。
        并且在消费端，Kafka 保证，1个 partition 只能被1个 consumer 消费。或者你指定 key（比如 order id），具有同1个 key 的所有消息，会发往同1个 partition。也是有序的。



        max.in.flight.requests.per.connection该参数指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。
        把它设为 1 可以保证消息是按照发送的顺序写入服务器的，即使发生了重试。这种场景下无法保障单一partition的有序，一般来说要保障消息的有序性，
        对于消息的可靠性也是有要求的，所以一般retries可以设置为大于0，但是max.in.flight.requests.per.connection设置为1即可，不过这样就有一个问题，导致了消息的吞吐量大大降低。

        场景二：需要提升吞吐量max.in.flight.requests.per.connection设置大于1此场景下业务要保障消息的吞吐量，
        那么max.in.flight.requests.per.connection必然就会选择更大的一个阈值，但是此场景还能保障消息有序性吗？
        答案是肯定的，可以设置enable.idempotence=true，开启生产者的幂等生产，可以解决顺序性问题，并且允许max.in.flight.requests.per.connection设置大于1
        生产者幂等性
    
        引入幂等性引入目的：生产者重复生产消息。生产者进行retry会产生重试时，会重复产生消息。有了幂等性之后，在进行retry重试时，只会生成一个消息。
        1.2 幂等性实现1.2.1 PID 和 Sequence Number为了实现Producer的幂等性，Kafka引入了Producer ID（即PID）和Sequence Number。PID。
        每个新的Producer在初始化的时候会被分配一个唯一的PID，这个PID对用户是不可见的。Sequence Numbler。（
        对于每个PID，该Producer发送数据的每个<Topic, Partition>都对应一个从0开始单调递增的Sequence Number。
        Broker端在缓存中保存了这seq number，对于接收的每条消息，如果其序号比Broker缓存中序号大于1则接受它，
        否则将其丢弃。这样就可以实现了消息重复提交了。但是，只能保证单个Producer对于同一个<Topic, Partition>的Exactly Once语义。不能保证同一个Producer一个topic不同的partion幂等。

        kafka事务属性是指一系列的生产者生产消息和消费者提交偏移量的操作在一个事务，或者说是是一个原子操作），同时成功或者失败。

        在事务属性之前先引入了生产者幂等性，它的作用为：
        （1）生产者多次发送消息可以封装成一个原子操作，要么都成功，要么失败
        （2）consumer-transform-producer模式下，因为消费者提交偏移量出现问题，导致在重复消费消息时，
        生产者重复生产消息。需要将这个模式下消费者提交偏移量操作和生成者一系列生成消息的操作封装成一个原子操作。
        消费者提交偏移量导致重复消费消息的场景：消费者在消费消息完成提交偏移量o2之前挂掉了（假设它最近提交的偏移量是o1），此时执行再均衡时，其它消费者会重复消费消息(o1到o2之间的消息）。


    选举

    3 Kafka Partition选主机制

        利用Zookeeper的分布式锁，先生成controller，controller再从ISR选择leader

        1.
        Kafka的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。
        controller会将Leader的改变直接通过RPC的方式(比ZooKeeper Queue的方式更高效)通知需为此作为响应的Broker。

        没有使用 zk，所以无 2.3 问题；也没有注册 watch无 2.2 问题
        leader 失败了，就通过 controller 继续重新选举即可，所以克服所有问题。

        2. Kafka集群controller的选举

        每个Broker都会在Controller Path (/controller)上注册一个Watch。 当前
        Controller失败时，对应的Controller Path会自动消失(因为它是ephemeral
        Node)，此时该Watch被fire，所有“活” 着的Broker都会去竞选成为新的
        Controller (创建新的Controller Path)，但是只会有一个竞选成功(这点由
        Zookeeper保证)。竞选成功者即为新的Leader，竞选失败者则重新在新的
        Controller Path上注册Watch。因为Zookeeper的Watch是一次性的， 被fire一次
        之后即失效，所以需要重新注册。
        3. 由controller执行：
        从Zookeeper中读取当前分区的所有ISR(in-sync replicas)集合
        调用配置的分区选择算法选择分区的leader
